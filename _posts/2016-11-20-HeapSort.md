---
layout: post
title: HeapSort
description: "HeapSort"
headline: "Let's Sort it up: the revenge"
categories: algorithms
tags: 
  - algorithms
  - recursive sort
  - heap sort
  - sorting
  - recursion
comments: true
mathjax: null
featured: true
published: true
---
### Sorting Algorithms - Part 3
Let's continue with the study of another recursive sorting algorithm: the **HeapSort**.

### HeapSort
Like the [QuickSort]() and the [MergeSort](), the HeapSort is one of the efficient sorting algorithms based on 
comparison. It is efficient because the average performance of these methods hit the $$O(n⋅log_2n)$$ runtime, though 
with differences in the worst-case. Other inefficient algorithms, such as Selection, Insertion and Bubble Sort, 
have $$O(n^2)$$ complexity meaning that they make all the possible comparisons of the elements under analysis.

The HeapSort is an improvement of the SelectionSort but has a better worst-case scenario thanks to the leverage of an
efficient structure that allows the fast selection of the minimum (or maximum) of the data pool.
To understand the HeapSort we need to describe this data structure (the _heap_), a method to create one in a 
"reasonable" time, a method to keep its characteristics after a change and a method to translate the _heap_ in a 
sorted collection. Furthermore, it is necessary that the total time needed for these methods are less than the 
infamous $$O(n^2)$$ but possible near the lower bound of the comparison sorting algorithms $$\Omega(n⋅log_2n)$$ 
(spoiler: it will be!).

### The _Heap_
The _heap_ is a binary tree-based data structure that satisfied three properties:

1. is a completed binary tree (all levels are filled except possibly the last and all nodes in the last level are as 
far left as possible);
2. every data element considered is associated to one and only one node, and one node is associated to one and only 
one element;
3. every value of a node is always greater or equals of the values of the children of that node.

An example of an _heap_ with values from 1 to 100:

<img class="image-post" src="{{ site.url }}/images/heapsort/max-heap.png" alt="Heap" style="max-height: 300px;">

An _heap_ with $$n$$ nodes has an height of $$\Theta(log_2n)$$.
It can be easily represented by an array in which if the root of the tree is placed in the index ```i = 0```:

* the left child is placed in ```2i + 1```;
* the right child is placed in ```2i + 2```;
* the parent is placed in ```(i - 1) / 2``` always rounded to the small leading integer.

### Erase the max value
Suppose we already have an _heap_ with all the properties met. It is immediate to see that, for the property 3, the 
maximum value is placed in the root. Sooner or later, for sorting purpose, we would like to remove that element and 
be sure that all the properties of the _heap_ will still be verified. Than it is necessary a method capable to 
"fixing" the _heap_. This means that not only one of the root children must became the new root, but also that the 
shift cannot cause the ceasing of property 1.
 
An handy solution can be swapping the value in the root with the last element ```k``` of the _heap_ and erase it. Doing
that the property 1 is respected but not the property 3. So the method must "pull down" ```k``` and, during the 
process, bring a new max value in the root. The pseudo code for the ```fixHeap``` is shown below:
 
```
fixHeap(node n, heap h)
    if (n is a leaf of h) then return
    else
        c = max(childLeft, childRight)
        if (n < c)
            swap(n, c)
            fixHeap(c, h)
```

The max number of comparisons is given by the height of the tree: $$O(log_2n)$$.

### Building the _Heap_
Given an array of values, it is possible to build an _heap_ in a convenient time? Yes it is and to do so it is possible 
to use two already seen methods: the first one is divide and conquer and the second one is ```fixHeap```.
The creation of the tree by a positional array is an operation with no comparisons required. Then the array must 
fulfill the 3 properties of the _heap_. The second one is already verified having chosen the positional array as 
representation. To satisfy the others it is possible to divide the _heap_ is sub trees similar to the full tree and 
with the same properties and then apply the fix seen before. The pseudo code:

```
headpify(Tree T)
    if (T is empty) the return
    heapify(leftChild(T))
    heapify(rightChild(T))
    fixHeap(root(T), T)
```

But what about the time $$T(n)$$ needed by this algorithm? Every recursive call needs $$T(\frac{n}{2})$$ and we already 
seen 
that ```fixHeap``` needs $$O(log_2n)$$. So:

$$T(n) = 2T(\frac{n}{2}) + O(log_2n)$$

The [master theorem](https://en.wikipedia.org/wiki/Master_theorem) says that, in this case: $$T(n) = O(n)$$.

### Sorting in place
Let's put together the pieces seen until now. Given an array A, if exists a method to create in time $$t_1$$ a data 
structure in which finding and erasing the maximum require time $$t_2$$ than it is possible to sorting in time $$O
(t_1 + n⋅t_2)$$. Using the _heap_: $$t_1 = O(n)$$ and $$t_2 = O(log_2n)$$ so:

$$O(n + n⋅log_2n) = O(n⋅log_2n)$$

```
heapSort(array A, count)
    heapify(A)
    end <- count - 1
    while end > 0 do
        swap(A[end], A[0])
        end <- end - 1
        deleteLastNode(A)
        fixHeap(A)
```

Given a generic array the first thing to do is build the _heap_; then the first element (which is the max value) is 
put at the end of the array and removed from the _heap_; the _heap_ is then "fixed" and the new max value at the 
beginning is put in the new end just before the previous. The loop continues until the full sorting.